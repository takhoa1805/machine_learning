{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6eed98b",
   "metadata": {},
   "source": [
    "MODEL SELECTION\n",
    "\n",
    "Student depression prediction is a classification problem, and require a supervised machine learning algorithm for doing such task.\n",
    "\n",
    "Things to consider when choosing a machine learning algorithm:\n",
    "- Final goal: accuracy, speed, scalability\n",
    "- Data nature: outliters, size, quality, characteristic\n",
    "- Our data is the combination of both categorical data (Ex: Gender, Sleep Duration,...) and numerical data (CGPA, Work Hour,...)\n",
    "- Constraints: such as computational limitations\n",
    "\n",
    "\n",
    "\n",
    "There are serveral supervised machine learning algorithm that can be used for our task, such as:\n",
    "- Logistic Regression:\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- Support Vector Machine\n",
    "- Naive Bayes\n",
    "\n",
    "\n",
    "As this is a classification problem, we use Accuracy, Recall, Precision, F1 Score to measure our model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab7d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70632bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "    }\n",
    "\n",
    "def train_and_evaluate_all_models(base_filename, target_column, n_folds=5):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000,n_jobs=-1),\n",
    "        \"K-Nearest Neighbors\": KNeighborsClassifier(weights='distance',n_jobs=-1),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "        \"AdaBoost\": AdaBoostClassifier(),\n",
    "        # \"Voting Classifier\": VotingClassifier(estimators=[\n",
    "        #     ('lr', LogisticRegression(max_iter=1000, n_jobs=-1)),\n",
    "        #     ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "        #     ('svc', SVC(probability=True))\n",
    "        # ], voting='soft',n_jobs=-1),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(criterion='entropy'),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "        \"Support Vector Machine\": SVC(),\n",
    "        \"Naive Bayes\": GaussianNB()\n",
    "    }\n",
    "\n",
    "    results = {name: [] for name in models}\n",
    "\n",
    "    for fold in range(1, n_folds + 1):\n",
    "        print(f\"\\nüìÅ Fold {fold}\")\n",
    "        train_path = f\"../datasets/train/train_fold{fold}_{base_filename}\"\n",
    "        test_path = f\"../datasets/test/test_fold{fold}_{base_filename}\"\n",
    "\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        X_train = train_df.drop(columns=[target_column])\n",
    "        y_train = train_df[target_column]\n",
    "        X_test = test_df.drop(columns=[target_column])\n",
    "        y_test = test_df[target_column]\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            metrics = evaluate_model(y_test, y_pred)\n",
    "            results[model_name].append(metrics)\n",
    "\n",
    "            print(f\"üîπ {model_name}\")\n",
    "            print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "            print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1 Score:  {metrics['f1_score']:.4f}\")\n",
    "\n",
    "    # Calculate average metrics\n",
    "    print(\"\\nüìä Average Performance Across Folds:\")\n",
    "    for model_name in models:\n",
    "        avg_metrics = {\n",
    "            metric: sum(r[metric] for r in results[model_name]) / n_folds\n",
    "            for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    "        }\n",
    "        print(f\"\\nüîπ {model_name}\")\n",
    "        for metric, score in avg_metrics.items():\n",
    "            print(f\"  {metric.capitalize()}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d3980f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Fold 1\n",
      "üîπ Logistic Regression\n",
      "  Accuracy:  0.8357\n",
      "  Precision: 0.8351\n",
      "  Recall:    0.8357\n",
      "  F1 Score:  0.8351\n",
      "üîπ K-Nearest Neighbors\n",
      "  Accuracy:  0.7866\n",
      "  Precision: 0.7855\n",
      "  Recall:    0.7866\n",
      "  F1 Score:  0.7848\n",
      "üîπ Gradient Boosting\n",
      "  Accuracy:  0.8319\n",
      "  Precision: 0.8313\n",
      "  Recall:    0.8319\n",
      "  F1 Score:  0.8313\n",
      "üîπ AdaBoost\n",
      "  Accuracy:  0.8324\n",
      "  Precision: 0.8319\n",
      "  Recall:    0.8324\n",
      "  F1 Score:  0.8315\n",
      "üîπ Decision Tree\n",
      "  Accuracy:  0.7691\n",
      "  Precision: 0.7695\n",
      "  Recall:    0.7691\n",
      "  F1 Score:  0.7692\n",
      "üîπ Random Forest\n",
      "  Accuracy:  0.8276\n",
      "  Precision: 0.8269\n",
      "  Recall:    0.8276\n",
      "  F1 Score:  0.8269\n",
      "üîπ Support Vector Machine\n",
      "  Accuracy:  0.8362\n",
      "  Precision: 0.8358\n",
      "  Recall:    0.8362\n",
      "  F1 Score:  0.8352\n",
      "üîπ Naive Bayes\n",
      "  Accuracy:  0.6649\n",
      "  Precision: 0.7758\n",
      "  Recall:    0.6649\n",
      "  F1 Score:  0.6517\n",
      "\n",
      "üìÅ Fold 2\n",
      "üîπ Logistic Regression\n",
      "  Accuracy:  0.8420\n",
      "  Precision: 0.8414\n",
      "  Recall:    0.8420\n",
      "  F1 Score:  0.8412\n",
      "üîπ K-Nearest Neighbors\n",
      "  Accuracy:  0.7860\n",
      "  Precision: 0.7847\n",
      "  Recall:    0.7860\n",
      "  F1 Score:  0.7842\n",
      "üîπ Gradient Boosting\n",
      "  Accuracy:  0.8436\n",
      "  Precision: 0.8430\n",
      "  Recall:    0.8436\n",
      "  F1 Score:  0.8430\n",
      "üîπ AdaBoost\n",
      "  Accuracy:  0.8422\n",
      "  Precision: 0.8416\n",
      "  Recall:    0.8422\n",
      "  F1 Score:  0.8414\n",
      "üîπ Decision Tree\n",
      "  Accuracy:  0.7615\n",
      "  Precision: 0.7609\n",
      "  Recall:    0.7615\n",
      "  F1 Score:  0.7611\n",
      "üîπ Random Forest\n",
      "  Accuracy:  0.8341\n",
      "  Precision: 0.8334\n",
      "  Recall:    0.8341\n",
      "  F1 Score:  0.8335\n",
      "üîπ Support Vector Machine\n",
      "  Accuracy:  0.8453\n",
      "  Precision: 0.8448\n",
      "  Recall:    0.8453\n",
      "  F1 Score:  0.8442\n",
      "üîπ Naive Bayes\n",
      "  Accuracy:  0.4177\n",
      "  Precision: 0.7331\n",
      "  Recall:    0.4177\n",
      "  F1 Score:  0.2552\n",
      "\n",
      "üìÅ Fold 3\n",
      "üîπ Logistic Regression\n",
      "  Accuracy:  0.8447\n",
      "  Precision: 0.8441\n",
      "  Recall:    0.8447\n",
      "  F1 Score:  0.8442\n",
      "üîπ K-Nearest Neighbors\n",
      "  Accuracy:  0.7889\n",
      "  Precision: 0.7876\n",
      "  Recall:    0.7889\n",
      "  F1 Score:  0.7872\n",
      "üîπ Gradient Boosting\n",
      "  Accuracy:  0.8445\n",
      "  Precision: 0.8440\n",
      "  Recall:    0.8445\n",
      "  F1 Score:  0.8442\n",
      "üîπ AdaBoost\n",
      "  Accuracy:  0.8418\n",
      "  Precision: 0.8412\n",
      "  Recall:    0.8418\n",
      "  F1 Score:  0.8412\n",
      "üîπ Decision Tree\n",
      "  Accuracy:  0.7654\n",
      "  Precision: 0.7657\n",
      "  Recall:    0.7654\n",
      "  F1 Score:  0.7656\n",
      "üîπ Random Forest\n",
      "  Accuracy:  0.8369\n",
      "  Precision: 0.8363\n",
      "  Recall:    0.8369\n",
      "  F1 Score:  0.8364\n",
      "üîπ Support Vector Machine\n",
      "  Accuracy:  0.8433\n",
      "  Precision: 0.8427\n",
      "  Recall:    0.8433\n",
      "  F1 Score:  0.8425\n",
      "üîπ Naive Bayes\n",
      "  Accuracy:  0.6302\n",
      "  Precision: 0.7606\n",
      "  Recall:    0.6302\n",
      "  F1 Score:  0.5239\n",
      "\n",
      "üìÅ Fold 4\n",
      "üîπ Logistic Regression\n",
      "  Accuracy:  0.8429\n",
      "  Precision: 0.8423\n",
      "  Recall:    0.8429\n",
      "  F1 Score:  0.8424\n",
      "üîπ K-Nearest Neighbors\n",
      "  Accuracy:  0.7880\n",
      "  Precision: 0.7867\n",
      "  Recall:    0.7880\n",
      "  F1 Score:  0.7865\n",
      "üîπ Gradient Boosting\n",
      "  Accuracy:  0.8432\n",
      "  Precision: 0.8427\n",
      "  Recall:    0.8432\n",
      "  F1 Score:  0.8428\n",
      "üîπ AdaBoost\n",
      "  Accuracy:  0.8434\n",
      "  Precision: 0.8429\n",
      "  Recall:    0.8434\n",
      "  F1 Score:  0.8429\n",
      "üîπ Decision Tree\n",
      "  Accuracy:  0.7694\n",
      "  Precision: 0.7704\n",
      "  Recall:    0.7694\n",
      "  F1 Score:  0.7698\n",
      "üîπ Random Forest\n",
      "  Accuracy:  0.8360\n",
      "  Precision: 0.8354\n",
      "  Recall:    0.8360\n",
      "  F1 Score:  0.8355\n",
      "üîπ Support Vector Machine\n",
      "  Accuracy:  0.8416\n",
      "  Precision: 0.8410\n",
      "  Recall:    0.8416\n",
      "  F1 Score:  0.8410\n",
      "üîπ Naive Bayes\n",
      "  Accuracy:  0.8170\n",
      "  Precision: 0.8209\n",
      "  Recall:    0.8170\n",
      "  F1 Score:  0.8129\n",
      "\n",
      "üìÅ Fold 5\n",
      "üîπ Logistic Regression\n",
      "  Accuracy:  0.8461\n",
      "  Precision: 0.8459\n",
      "  Recall:    0.8461\n",
      "  F1 Score:  0.8453\n",
      "üîπ K-Nearest Neighbors\n",
      "  Accuracy:  0.7974\n",
      "  Precision: 0.7967\n",
      "  Recall:    0.7974\n",
      "  F1 Score:  0.7957\n",
      "üîπ Gradient Boosting\n",
      "  Accuracy:  0.8492\n",
      "  Precision: 0.8489\n",
      "  Recall:    0.8492\n",
      "  F1 Score:  0.8484\n",
      "üîπ AdaBoost\n",
      "  Accuracy:  0.8465\n",
      "  Precision: 0.8465\n",
      "  Recall:    0.8465\n",
      "  F1 Score:  0.8454\n",
      "üîπ Decision Tree\n",
      "  Accuracy:  0.7755\n",
      "  Precision: 0.7749\n",
      "  Recall:    0.7755\n",
      "  F1 Score:  0.7751\n",
      "üîπ Random Forest\n",
      "  Accuracy:  0.8440\n",
      "  Precision: 0.8436\n",
      "  Recall:    0.8440\n",
      "  F1 Score:  0.8432\n",
      "üîπ Support Vector Machine\n",
      "  Accuracy:  0.8512\n",
      "  Precision: 0.8512\n",
      "  Recall:    0.8512\n",
      "  F1 Score:  0.8502\n",
      "üîπ Naive Bayes\n",
      "  Accuracy:  0.7161\n",
      "  Precision: 0.7921\n",
      "  Recall:    0.7161\n",
      "  F1 Score:  0.7108\n",
      "\n",
      "üìä Average Performance Across Folds:\n",
      "\n",
      "üîπ Logistic Regression\n",
      "  Accuracy: 0.8423\n",
      "  Precision: 0.8418\n",
      "  Recall: 0.8423\n",
      "  F1_score: 0.8416\n",
      "\n",
      "üîπ K-Nearest Neighbors\n",
      "  Accuracy: 0.7894\n",
      "  Precision: 0.7882\n",
      "  Recall: 0.7894\n",
      "  F1_score: 0.7877\n",
      "\n",
      "üîπ Gradient Boosting\n",
      "  Accuracy: 0.8425\n",
      "  Precision: 0.8420\n",
      "  Recall: 0.8425\n",
      "  F1_score: 0.8419\n",
      "\n",
      "üîπ AdaBoost\n",
      "  Accuracy: 0.8413\n",
      "  Precision: 0.8408\n",
      "  Recall: 0.8413\n",
      "  F1_score: 0.8405\n",
      "\n",
      "üîπ Decision Tree\n",
      "  Accuracy: 0.7682\n",
      "  Precision: 0.7683\n",
      "  Recall: 0.7682\n",
      "  F1_score: 0.7682\n",
      "\n",
      "üîπ Random Forest\n",
      "  Accuracy: 0.8357\n",
      "  Precision: 0.8351\n",
      "  Recall: 0.8357\n",
      "  F1_score: 0.8351\n",
      "\n",
      "üîπ Support Vector Machine\n",
      "  Accuracy: 0.8435\n",
      "  Precision: 0.8431\n",
      "  Recall: 0.8435\n",
      "  F1_score: 0.8426\n",
      "\n",
      "üîπ Naive Bayes\n",
      "  Accuracy: 0.6492\n",
      "  Precision: 0.7765\n",
      "  Recall: 0.6492\n",
      "  F1_score: 0.5909\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_all_models(\n",
    "    base_filename=\"preprocessed_student_depression.csv\",\n",
    "    target_column=\"Depression\",  # change to your actual label column\n",
    "    n_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ec1ba",
   "metadata": {},
   "source": [
    "CHOOSING METRIC TO DECIDE WHICH MODEL WORKS BEST FOR DATA\n",
    "\n",
    "- As our goal is to predict if a student has a depression or not, we can tolerate wrong predictions for students who do not have depression (false positive), rather than leaving possible depression cases predicted 'not depression'. From that point, we can have early mental health warning for students that could possibly has depression.\n",
    "- So to choose the best suitable model for the task, we choose the model that has highest \"Recall\" in terms of performance metric\n",
    "\n",
    "\n",
    "===> We choose Support Vector Machine model for the task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
